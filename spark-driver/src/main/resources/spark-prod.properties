# Mesos via Zookeeper (in case of multi-master deployment)
#spark.master=mesos://zk://master.mesos:2181/mesos
# Mesos directly (single master deployment)
spark.master=mesos://master.mesos:5050

# Spark application name.
spark.application-name=SparkBoot

# Path to distributed library that should be loaded into each worker of a Spark cluster.
# NOTE: hdfs://hdfs/ - is a correct URL for base path of the DC/OS (Mesos) deployment of the HDFS service
# TODO: Test driver-side distribution feature (however, this may be a poor choice for large deployments).
spark.distributed-libraries=hdfs://hdfs/jars/spark-distributed-library-1.0-SNAPSHOT.jar

# Amount of memory to use for the driver process.
spark.driver.memory=512m


### Start of Mesos ###
spark.mesos.coarse=true
# Download hdfs configurations from DC/OS (Mesos) deployment of the HDFS service
# (NOTE: .mesos is the default domain zone for Mesos)
spark.mesos.uris=http://master.mesos/service/hdfs/v1/connect/hdfs-site.xml,http://master.mesos/service/hdfs/v1/connect/core-site.xml

# Enable history service
# NOTE: Set to false, due to the requirement of creating history folder manually
spark.eventLog.enabled=false
# NOTE: The folder should be already created on HDFS
spark.eventLog.dir=hdfs://hdfs/history

# Executor
# NOTE: This is a default Docker container for spawning Spark slaves on Mesos (it contains both Spark and Mesos library *.so files with all dependencies)
spark.mesos.executor.docker.image=mesosphere/spark:1.0.2-2.0.0
spark.mesos.executor.home=/opt/spark/dist
spark.executor.uri=http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.6.tgz

# The maximum amount of CPU cores across cluster
# NOTE: 1 is a lowest value. The value should be of Integer type.
# NOTE: If this property and spark.executory.core are set to the same value, then the cluster will consist of a single worker/executor.
spark.cores.max=1

# The maximum amount of CPU cores to request for one executor
# NOTE: 1 is a lowest value. The value should be of Integer type.
spark.executor.cores=1

# Amount of memory to assign to each executor process
spark.executor.memory=2g
### End of Mesos ###


# The largest number of partitions in a parent RDD during distributed shuffle operations.
# For local mode should be equal to number of cores on the local machine.
spark.default.parallelism=4

# Serializer: org.apache.spark.serializer.JavaSerializer (default) or org.apache.spark.serializer.KryoSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryoserializer.buffer.max=128m

# The number of partitions to use when shuffling data for joins or aggregations.
spark.sql.shuffle.partitions=5
